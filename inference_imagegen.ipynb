{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Generator\n",
    "\n",
    "This notebook will load the CXR8 data, load a model, and then perform inference on the model and output the result to a temp directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\data\\cxr8\\filtered\\no finding\\00000002_000.png torch.Size([1, 448, 448])\n",
      "D:\\data\\cxr8\\filtered\\no finding\\00000005_000.png torch.Size([1, 448, 448])\n",
      "D:\\data\\cxr8\\filtered\\no finding\\00000005_001.png torch.Size([1, 448, 448])\n",
      "D:\\data\\cxr8\\filtered\\no finding\\00000005_002.png torch.Size([1, 448, 448])\n",
      "D:\\data\\cxr8\\filtered\\no finding\\00000005_003.png torch.Size([1, 448, 448])\n",
      "D:\\data\\cxr8\\filtered\\no finding\\00000005_004.png torch.Size([1, 448, 448])\n",
      "D:\\data\\cxr8\\filtered\\no finding\\00000005_005.png torch.Size([1, 448, 448])\n",
      "D:\\data\\cxr8\\filtered\\no finding\\00000006_000.png torch.Size([1, 448, 448])\n",
      "D:\\data\\cxr8\\filtered\\no finding\\00000007_000.png torch.Size([1, 448, 448])\n",
      "D:\\data\\cxr8\\filtered\\no finding\\00000008_001.png torch.Size([1, 448, 448])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from load_dataset import load_dataset\n",
    "\n",
    "INPUT_SIZE = (1, 448, 448)\n",
    "infer_dataset = load_dataset(\n",
    "    \"cxr8\",\n",
    "    input_size=INPUT_SIZE,\n",
    "    clahe_tile_size=8,\n",
    ")\n",
    "\n",
    "for n in range(min(10, len(infer_dataset))):\n",
    "    print(f\"{infer_dataset.imgs[n][0]} {infer_dataset[n][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make_oriented_map: weights_real.shape = torch.Size([40, 1, 11, 11])\n",
      "make_oriented_map: weights_real.shape = torch.Size([40, 1, 11, 11])\n",
      "self.input_size_to_fc = [512, 7, 7]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VAE:\n\tMissing key(s) in state_dict: \"localization.0.weight\", \"localization.0.bias\", \"localization.3.weight\", \"localization.3.bias\", \"fc_rot.0.weight\", \"fc_rot.0.bias\", \"fc_rot.2.weight\", \"fc_rot.2.bias\", \"fc_xlate.0.weight\", \"fc_xlate.0.bias\", \"fc_xlate.2.weight\", \"fc_xlate.2.bias\". \n\tUnexpected key(s) in state_dict: \"localization.2.weight\", \"localization.2.bias\", \"fc_lut.0.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39m# model = VAE((1 if dataset_name == \"cxr8\" else 3, 224, 224), latent_dim).to(device)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m model \u001b[39m=\u001b[39m VAE(INPUT_SIZE, init_kernel_size\u001b[39m=\u001b[39mKERNEL_SIZE, latent_dim\u001b[39m=\u001b[39mLATENT_DIM)\n\u001b[1;32m---> 13\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mweights/20230425175434_clahe8_kernel11_latent32_orisq.zip\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[0;32m     14\u001b[0m \u001b[39mif\u001b[39;00m show_summary:\n\u001b[0;32m     15\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m     16\u001b[0m         summary(\n\u001b[0;32m     17\u001b[0m             model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m         )\n\u001b[0;32m     29\u001b[0m     )\n",
      "File \u001b[1;32mc:\\dev\\resnet18-vae\\venv39\\lib\\site-packages\\torch\\nn\\modules\\module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   2036\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[0;32m   2037\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2038\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2040\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2041\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   2042\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2043\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VAE:\n\tMissing key(s) in state_dict: \"localization.0.weight\", \"localization.0.bias\", \"localization.3.weight\", \"localization.3.bias\", \"fc_rot.0.weight\", \"fc_rot.0.bias\", \"fc_rot.2.weight\", \"fc_rot.2.bias\", \"fc_xlate.0.weight\", \"fc_xlate.0.bias\", \"fc_xlate.2.weight\", \"fc_xlate.2.bias\". \n\tUnexpected key(s) in state_dict: \"localization.2.weight\", \"localization.2.bias\", \"fc_lut.0.bias\". "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchinfo import summary\n",
    "\n",
    "from vae import VAE, vae_loss\n",
    "\n",
    "KERNEL_SIZE = 11\n",
    "DIRECTIONS = 7\n",
    "LATENT_DIM = 32  # 64\n",
    "show_summary = True\n",
    "\n",
    "# model = VAE((1 if dataset_name == \"cxr8\" else 3, 224, 224), latent_dim).to(device)\n",
    "model = VAE(INPUT_SIZE, init_kernel_size=KERNEL_SIZE, latent_dim=LATENT_DIM)\n",
    "model.load_state_dict(torch.load(\"weights/20230425175434_clahe8_kernel11_latent32_orisq.zip\"))\n",
    "if show_summary:\n",
    "    print(\n",
    "        summary(\n",
    "            model,\n",
    "            input_size=(37, INPUT_SIZE[0], INPUT_SIZE[1], INPUT_SIZE[2]),\n",
    "            depth=10,\n",
    "            col_names=[\n",
    "                \"input_size\",\n",
    "                \"kernel_size\",\n",
    "                \"mult_adds\",\n",
    "                \"num_params\",\n",
    "                \"output_size\",\n",
    "                \"trainable\",\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "model = model.to(device)\n",
    "print(set([p.device for p in model.parameters()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(20,8))\n",
    "\n",
    "for n in range(min(5, len(infer_dataset))):\n",
    "\n",
    "    x_input = infer_dataset[n+1000][0]\n",
    "    x_input = torch.unsqueeze(x_input, 0)\n",
    "    x_input = x_input.cuda()\n",
    "    print(f\"x_input.shape = {x_input.shape}\")\n",
    "\n",
    "    warp, lut = model.stn_grid_lut(x_input)\n",
    "\n",
    "    lut = lut.cpu().detach()\n",
    "    print(f\"lut = {lut.numpy()}\")\n",
    "\n",
    "    warp = warp.cpu().detach()\n",
    "    print(f\"warp = {warp.numpy()}\")\n",
    "\n",
    "    x_recon, mu, log_var = model(x_input)\n",
    "    print(f\"x_recon.shape = {x_recon.shape}\")\n",
    "\n",
    "    x_input = x_input.cpu().detach()\n",
    "    x_recon = x_recon.cpu().detach()\n",
    "\n",
    "    ax[0][n].imshow(torch.squeeze(x_input), cmap='bone')\n",
    "    ax[1][n].imshow(torch.squeeze(x_recon), cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
